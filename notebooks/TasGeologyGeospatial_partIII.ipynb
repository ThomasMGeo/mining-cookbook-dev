{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geospatial ML - Further Analysis - Part III"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Orignal Repo\n",
    "\n",
    "is [here](https://github.com/Solve-Geosolutions/transform_2022)\n",
    "\n",
    "### Original Data License\n",
    "\n",
    "All data presented in this tutorial were derived from open data sets made available through [Mineral Resources Tasmania](https://www.mrt.tas.gov.au/) and [Geoscience Australia](https://www.ga.gov.au/).\n",
    "\n",
    "**LICENSE CONDITIONS**\n",
    "\n",
    "By exporting this data you accept and comply with the terms and conditions set out below:\n",
    "\n",
    "[Creative Commons Attribution 3.0 Australia](https://creativecommons.org/licenses/by/3.0/au/)\n",
    "\n",
    "<img src=\"assets/creative_commons_logo.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why This Mining Geology Example?\n",
    "\n",
    "This is a great example of how we need to account for [spatial autocorrelation](https://www.paulamoraga.com/book-spatial/spatial-autocorrelation.html#) (another reference [here](https://mgimond.github.io/Spatial/spatial-autocorrelation.html)) for machine learning projects. Rocks generally (but not always!) are not time dependent. I would not necessairly use the same methods for time dependent weather/climate datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Roadmap\n",
    "\n",
    "  1. Load and inspect data sets\n",
    "  1. Combine data sets to build a labeled N<sub>pixel</sub>, N<sub>layers</sub> array for model training\n",
    "      - inspect differences between proximal vs. distal to mineralisation pixels      \n",
    "  1. Train a random GBDT classificer     \n",
    "  1. Develop a checkerboard data selection procedure, train and evaluate models\n",
    "      - discuss effects of spatially separated testing data\n",
    "  2. Grid Search for the classifier\n",
    "  3. Recursive Feature importance\n",
    "\n",
    "\n",
    " \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might need to add these two packages to your enviroment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rasterio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import key packages\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from rasterio.features import rasterize\n",
    "# Machine learning packages\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.feature_selection import RFE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_recall_curve\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from CheckmateSample.generator import make_checkerboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier, plot_importance\n",
    "\n",
    "\n",
    "print('XGboost version', xgb.__version__ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set some plotting parameters \n",
    "mpl.rcParams.update({\"axes.grid\":True, \"grid.color\":\"gray\", \"grid.linestyle\":'--','figure.figsize':(10,10)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pointing to our data directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'geodata/'\n",
    "\n",
    "# set path to minoccs\n",
    "point_fn = os.path.join(data_dir, 'sn_w_minoccs.gpkg')\n",
    "\n",
    "# make a list of rasters in the data directory\n",
    "geotiffs = [os.path.join(data_dir, x) for x in os.listdir(data_dir) if '.tif' in x]\n",
    "point_fn, geotiffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First cut at plotting, where do we have occurances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = gpd.read_file(point_fn)\n",
    "df.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in GeoTiffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read geotiffs\n",
    "data, names = [], []  # Lists to store data and corresponding file names\n",
    "for fn in geotiffs:  # Loop through each GeoTIFF file\n",
    "    with rasterio.open(fn, 'r') as src:  # Open GeoTIFF file for reading\n",
    "        # read spatial information\n",
    "        transform = src.transform  # Get affine transformation matrix\n",
    "        region = (src.bounds[0], src.bounds[2], src.bounds[1], src.bounds[3])  # Get bounding box coordinates (left, bottom, right, top)\n",
    "        # read band 1 data\n",
    "        d = src.read(1)  # Read data from the first band\n",
    "        nodata_mask = d == src.nodata  # Create a mask for NoData values\n",
    "        d[nodata_mask] = np.nan  # Replace NoData values with NaN\n",
    "        # append data to lists\n",
    "        data.append(d)  # Append data to the list\n",
    "        names.append(os.path.basename(fn).replace('.tif',''))  # Append file name to the list (without extension)\n",
    "\n",
    "# stack list into 3D numpy array\n",
    "data = np.stack(data)  # Stack the list of arrays into a 3D numpy array\n",
    "data.shape, names  # Return the shape of the data array and the list of file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the data\n",
    "fig, axes = plt.subplots(3,3,figsize=(12,18))\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    if i < data.shape[0]:\n",
    "        ax.imshow(data[i], vmin=np.nanpercentile(data[i], 5), vmax=np.nanpercentile(data[i], 95), extent=region)\n",
    "        ax.set(title=names[i])\n",
    "        df.plot(ax=ax, marker='*', facecolor='w')\n",
    "    else:\n",
    "        ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rasterize the point\n",
    "geometry_generator = ((geom, 1) for geom in df.buffer(1000).geometry)\n",
    "labels = rasterize(shapes=geometry_generator, out_shape=data[0].shape, fill=0, transform=transform).astype('float32')\n",
    "labels[nodata_mask] = np.nan\n",
    "\n",
    "## plt.imshow(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dimensions\n",
    "if np.shape(labels.flatten())[0] != np.shape(data.reshape((data.shape[0], data.shape[1] * data.shape[2])).T)[0]:\n",
    "    raise ValueError(\"Labels and Data shapes (0th dimmension) do not match.\")\n",
    "\n",
    "X_pix = data.reshape((data.shape[0], data.shape[1] * data.shape[2])).T\n",
    "y_pix = labels.flatten()\n",
    "\n",
    "# remove nans\n",
    "X = X_pix[~np.isnan(y_pix)]\n",
    "y = y_pix[~np.isnan(y_pix)]\n",
    "\n",
    "print(f\"Shape of data after removing NaNs (X): {X.shape}\")\n",
    "print(f\"Shape of labels after removing NaNs (y): {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpus = os.cpu_count()\n",
    "print(f'The number of available CPUs is: {cpus}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will generally use 1 less. I have found that it does not slow it down a ton, but makes the user experiance generally much better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate train and testing subsets\n",
    "We will use the checkerboard example from the previous notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to get probability map\n",
    "def get_proba_map(X_pix, nodata_mask, model):\n",
    "    \"\"\"\n",
    "    Generates a probability map from input features using a trained model.\n",
    "    \n",
    "    Parameters:\n",
    "    X_pix (np.ndarray): A NumPy array containing the input features for prediction. Each row represents a pixel, and each column represents a feature.\n",
    "    nodata_mask (np.ndarray): A boolean array with the same shape as the first dimension of X_pix. True values indicate pixels with no data (nodata).\n",
    "    model (sklearn.base.BaseEstimator): A trained scikit-learn model that supports the predict_proba method.\n",
    "    feature_names (list of str): A list of feature names corresponding to the columns in X_pix.\n",
    "    \n",
    "    Returns:\n",
    "    np.ndarray: A 2D array with the same shape as nodata_mask, containing the predicted probabilities for each pixel. Pixels with no data are assigned NaN.\n",
    "    \"\"\"\n",
    "    # Remove nulls by filtering out pixels where nodata_mask is True\n",
    "    X = X_pix[np.invert(nodata_mask.flatten())]\n",
    "    \n",
    "    # Get predictions from the model (probability of the positive class)\n",
    "    predictions = model.predict_proba(X)[:, 1]\n",
    "    \n",
    "    # Create an output array initialized to zeros, with the same shape as the flattened nodata_mask\n",
    "    pred_ar = np.zeros(shape=nodata_mask.flatten().shape, dtype='float32')\n",
    "    \n",
    "    # Insert predictions into the output array at positions where nodata_mask is False\n",
    "    pred_ar[np.invert(nodata_mask.flatten())] = predictions\n",
    "    \n",
    "    # Reshape the output array to match the original shape of nodata_mask\n",
    "    pred_ar = pred_ar.reshape(nodata_mask.shape)\n",
    "    \n",
    "    # Assign NaN to positions where nodata_mask is True\n",
    "    pred_ar[nodata_mask] = np.nan\n",
    "    \n",
    "    return pred_ar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap questions:\n",
    "\n",
    "What are the big differences between the first and second model?\n",
    "\n",
    "Which one do you like more? \n",
    "\n",
    "Any downsides to this methodology?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to (try to) overcome spatial autocorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rus = RandomUnderSampler(random_state=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make checkerboard\n",
    "checker = make_checkerboard(data[0].shape, (400,400))\n",
    "checker[nodata_mask] = np.nan\n",
    "\n",
    "#plot checkerboard\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "ax.imshow(checker, extent=region)\n",
    "df.plot(ax=ax, marker='*', facecolor='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data into checkers\n",
    "X_check0 = X_pix[checker.flatten()==0]\n",
    "y_check0 = y_pix[checker.flatten()==0]\n",
    "\n",
    "X_check1 = X_pix[checker.flatten()==1]\n",
    "y_check1 = y_pix[checker.flatten()==1]\n",
    "\n",
    "# remove nans\n",
    "X_check0 = X_check0[~np.isnan(y_check0)]\n",
    "y_check0 = y_check0[~np.isnan(y_check0)]\n",
    "\n",
    "X_check1 = X_check1[~np.isnan(y_check1)]\n",
    "y_check1 = y_check1[~np.isnan(y_check1)]\n",
    "\n",
    "# print some details\n",
    "print ('Checker 0: X data array shape is {}, y labels array shape is {}'.format(X_check0.shape, y_check0.shape))\n",
    "print ('Checker 1: X data array shape is {}, y labels array shape is {}'.format(X_check1.shape, y_check1.shape))\n",
    "\n",
    "# run undersampling\n",
    "X_check0, y_check0 = rus.fit_resample(X_check0, y_check0)\n",
    "X_check1, y_check1 = rus.fit_resample(X_check1, y_check1)\n",
    "\n",
    "# print some details\n",
    "print ('Checker 0: X data array shape is {}, y labels array shape is {}'.format(X_check0.shape, y_check0.shape))\n",
    "print ('Checker 1: X data array shape is {}, y labels array shape is {}'.format(X_check1.shape, y_check1.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project, we will use th checkerboard method from the first notebook. Splitting up the labels would also be valid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit some models\n",
    "model0 = XGBClassifier(n_jobs=cpus-1)\n",
    "model1 = XGBClassifier(n_jobs=cpus-1)\n",
    "\n",
    "model0.fit(X_check0, y_check0)\n",
    "model1.fit(X_check1, y_check1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate probability maps\n",
    "pred_ar0 = get_proba_map(X_pix, nodata_mask, model0)\n",
    "pred_ar1 = get_proba_map(X_pix, nodata_mask, model1)\n",
    "\n",
    "# Calculate the difference map\n",
    "difference_map = pred_ar0 - pred_ar1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot probability maps\n",
    "fig, ax = plt.subplots(1, 3, figsize=(18, 10))\n",
    "for i, (ar, title) in enumerate(zip([pred_ar0, pred_ar1, difference_map], ['Checkerboard0', 'Checkerboard1', 'Difference'])):\n",
    "    if title == 'Difference':\n",
    "        im = ax[i].imshow(ar, extent=region)\n",
    "    else:\n",
    "        im = ax[i].imshow(ar, extent=region, vmin=0, vmax=1)\n",
    "    ax[i].set_title(title)\n",
    "    # df.plot(ax=ax[i], marker='*', facecolor='w')\n",
    "    fig.colorbar(im, ax=ax[i], orientation='vertical', fraction=0.064, pad=0.04)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results might look a little different than previously due to using XGBoost compared to RandomForest from scikit learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparmeter Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search for hyperparameters in machine learning is essential because it systematically explores a range of hyperparameter combinations to find the optimal set that improves model performance. Hyperparameters, such as learning rates, regularization strengths, and kernel parameters, significantly influence the model's learning process and predictive accuracy. A grid search helps ensure that we thoroughly investigate the parameter space, identifying the configuration that maximizes model performance while minimizing overfitting. This method, although computationally intensive, leads to more robust and reliable models by providing a structured approach to optimize their settings for the best possible results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a hyperparameter?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A hyperparameter is a configuration parameter used in machine learning models that is set before the training process begins and is not learned from the data. Unlike model parameters, which are internal variables learned from the data during training (like weights in a neural network), hyperparameters are external configurations that govern the training process itself and the model's architecture. They play a crucial role in shaping the behavior of the model and its learning capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A quick test, with no parameter tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_clf_test = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "xgb_clf_test.fit(X_check0, y_check0)\n",
    "preds = xgb_clf_test.predict(X_check1)\n",
    "accuracy_test = accuracy_score(y_check1, preds)\n",
    "print(f\"Test accuracy: {round(accuracy_test, 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice we are using one checkerboard to score the other. This is different than earlier where we just looked at raw probabilities for the overall map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's see if we can improve on that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize XGBoost classifier\n",
    "xgb_clf0 = xgb.XGBClassifier(use_label_encoder=False, \n",
    "                             eval_metric='logloss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use this param_grid again, so will not label it 0 or 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'max_depth': [2, 3, 4],\n",
    "    'learning_rate': [0.05, 0.01, 0.5],\n",
    "    'n_estimators': [25, 50, 100, 200],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up GridSearchCV\n",
    "grid_search0 = GridSearchCV(estimator=xgb_clf0, \n",
    "                           param_grid=param_grid, \n",
    "                           scoring='accuracy', \n",
    "                           cv=5, \n",
    "                           verbose=1, \n",
    "                           n_jobs=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search0.fit(X_check0, y_check0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best parameters found: {grid_search0.best_params_}\")\n",
    "print(f\"Best cross-validation score: {grid_search0.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "best_model = grid_search0.best_estimator_\n",
    "y_pred = best_model.predict(X_check1)\n",
    "accuracy = accuracy_score(y_check1, y_pred)\n",
    "print(f\"Test accuracy: {round(accuracy, 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('improvement using grid search:', np.round(accuracy - accuracy_test,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we will do the same thing, but flip-flop the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_clf_test1 = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "xgb_clf_test1.fit(X_check1, y_check1)\n",
    "preds = xgb_clf_test1.predict(X_check0)\n",
    "accuracy_test1 = accuracy_score(y_check0, preds)\n",
    "print(f\"Test accuracy: {round(accuracy_test1, 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize XGBoost classifier\n",
    "xgb_clf1 = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search1 = GridSearchCV(estimator=xgb_clf0, \n",
    "                           param_grid=param_grid, \n",
    "                           scoring='accuracy', \n",
    "                           cv=5, \n",
    "                           verbose=1, \n",
    "                           n_jobs=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search1.fit(X_check1, y_check1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best parameters found: {grid_search1.best_params_}\")\n",
    "print(f\"Best cross-validation score: {grid_search1.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "best_model1 = grid_search1.best_estimator_\n",
    "y_pred1 = best_model1.predict(X_check0)\n",
    "accuracy1 = accuracy_score(y_check0, y_pred1)\n",
    "print(f\"Test accuracy: {round(accuracy, 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few follow up questions:\n",
    "- Why is using the other checkerboard a good test?\n",
    "- How different are the various hyperparameters from the two searches?\n",
    "- Did the improvment suprise you from standard hyperparameters to best ones?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive Feature Elimination and Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(X_check0, columns=names)\n",
    "X_test = pd.DataFrame(X_check1, columns=names)\n",
    "\n",
    "y_train = pd.DataFrame(y_check0)\n",
    "y_test = pd.DataFrame(y_check1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create an XGBoost classifier\n",
    "model = XGBClassifier(max_depth=3, \n",
    "                      n_estimators=50,\n",
    "                      learning_rate=0.01)\n",
    "\n",
    "# Create an RFE object\n",
    "rfe = RFE(estimator=model)\n",
    "\n",
    "# Fit the RFE object to the training data\n",
    "rfe.fit(X_train, y_train)\n",
    "\n",
    "# Get the selected features\n",
    "selected_features = rfe.get_support(indices=True)\n",
    "\n",
    "# Train a new XGBoost classifier on the selected features\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train.iloc[:, selected_features], y_train)\n",
    "\n",
    "# Evaluate the model on the testing set\n",
    "y_pred = model.predict(X_test.iloc[:, selected_features])\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Calculate the accuracy for different number of features\n",
    "n_features = range(1, X_train.shape[1] + 1)\n",
    "accuracies = []\n",
    "for n in n_features:\n",
    "    selected_features = rfe.get_support(indices=True)[:n]\n",
    "    model = XGBClassifier()\n",
    "    model.fit(X_train.iloc[:, selected_features], y_train)\n",
    "    y_pred = model.predict(X_test.iloc[:, selected_features])\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracies.append(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances and sort them\n",
    "feature_importances = model.feature_importances_\n",
    "sorted_idx = np.argsort(feature_importances)[::-1]\n",
    "sorted_features = [X_train.columns[i] for i in sorted_idx]\n",
    "sorted_importances = feature_importances[sorted_idx]\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(range(len(sorted_importances)), sorted_importances)\n",
    "plt.yticks(range(len(sorted_importances)), sorted_features)\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Importance')\n",
    "plt.gca().invert_yaxis()  # Invert y-axis to display largest importance at the top\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the accuracy vs. number of features\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(n_features, accuracies)\n",
    "plt.scatter(n_features, accuracies, alpha=0.4)\n",
    "plt.xlabel('Number of features')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy vs. Number of features for XGBoost classification with RFE')\n",
    "\n",
    "# Set the x axis to every data point\n",
    "plt.xticks(n_features)\n",
    "\n",
    "# Flip the x-axis\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an XGBoost classifier for RFE\n",
    "base_model = XGBClassifier(max_depth=3, \n",
    "                           n_estimators=50,\n",
    "                           learning_rate=0.01)\n",
    "\n",
    "# Perform RFE once to rank all features\n",
    "rfe = RFE(estimator=base_model, n_features_to_select=1)\n",
    "rfe.fit(X_train, y_train)\n",
    "\n",
    "# Get the ranking of features\n",
    "feature_ranking = rfe.ranking_\n",
    "\n",
    "# Sort features by their RFE ranking\n",
    "rfe_sorted_features = [x for _, x in sorted(zip(feature_ranking, X_train.columns))]\n",
    "\n",
    "# Calculate the accuracy for different numbers of features\n",
    "n_features = range(1, X_train.shape[1] + 1)\n",
    "accuracies = []\n",
    "\n",
    "print(\"Number of features | Accuracy | Features (RFE order)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for n in n_features:\n",
    "    # Select top n features based on RFE ranking\n",
    "    selected_features = rfe_sorted_features[:n]\n",
    "    \n",
    "    # Train a new XGBoost classifier on the selected features\n",
    "    model = XGBClassifier()\n",
    "    model.fit(X_train[selected_features], y_train)\n",
    "    \n",
    "    # Evaluate the model on the testing set\n",
    "    y_pred = model.predict(X_test[selected_features])\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "    # Print the current number of features, accuracy, and the features used\n",
    "    print(f\"{n:^17} | {accuracy:.4f} | {', '.join(selected_features)}\")\n",
    "\n",
    "# Train a final XGBoost model on all features\n",
    "final_model = XGBClassifier()\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances from the final XGBoost model\n",
    "xgb_feature_importances = final_model.feature_importances_\n",
    "xgb_sorted_idx = np.argsort(xgb_feature_importances)[::-1]\n",
    "xgb_sorted_features = [X_train.columns[i] for i in xgb_sorted_idx]\n",
    "xgb_sorted_importances = xgb_feature_importances[xgb_sorted_idx]\n",
    "\n",
    "# Create subplots\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(24, 8))\n",
    "\n",
    "# Plot RFE feature ranking\n",
    "ax1.barh(range(len(rfe_sorted_features)), range(len(rfe_sorted_features), 0, -1))\n",
    "ax1.set_yticks(range(len(rfe_sorted_features)))\n",
    "ax1.set_yticklabels(rfe_sorted_features)\n",
    "ax1.set_xlabel('RFE Ranking (lower is better)')\n",
    "ax1.set_ylabel('Feature')\n",
    "ax1.set_title('RFE Feature Ranking')\n",
    "ax1.invert_yaxis()  # Invert y-axis to display best ranked at the top\n",
    "\n",
    "# Plot XGBoost feature importance\n",
    "ax2.barh(range(len(xgb_sorted_importances)), xgb_sorted_importances)\n",
    "ax2.set_yticks(range(len(xgb_sorted_importances)))\n",
    "ax2.set_yticklabels(xgb_sorted_features)\n",
    "ax2.set_xlabel('Importance')\n",
    "ax2.set_ylabel('Feature')\n",
    "ax2.set_title('XGBoost Feature Importance (from final model)')\n",
    "ax2.invert_yaxis()  # Invert y-axis to display largest importance at the top\n",
    "\n",
    "# Plot accuracy vs number of features\n",
    "ax3.plot(n_features, accuracies)\n",
    "ax3.scatter(n_features, accuracies, alpha=0.4)\n",
    "ax3.set_xlabel('Number of features')\n",
    "ax3.set_ylabel('Accuracy')\n",
    "ax3.set_title('Accuracy vs. Number of features (RFE order)')\n",
    "ax3.set_xticks(n_features[::5])  # Show every 5th feature on x-axis for readability\n",
    "ax3.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print the order of feature selection by RFE\n",
    "print(\"\\nOrder of feature selection by RFE:\")\n",
    "for i, feature in enumerate(rfe_sorted_features, 1):\n",
    "    print(f\"{i}. {feature}\")\n",
    "\n",
    "# Print the order of feature importance by XGBoost\n",
    "print(\"\\nOrder of feature importance by XGBoost:\")\n",
    "for i, feature in enumerate(xgb_sorted_features, 1):\n",
    "    print(f\"{i}. {feature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy in scikit-learn has several limitations for mining prediction problems:\n",
    "\n",
    "Class imbalance is common in mining datasets. Rare but critical events like high-grade ore deposits or geotechnical hazards may be overlooked by models that achieve high accuracy by always predicting the most common class.The fixed threshold in accuracy calculations doesn't reflect the complex trade-offs in mining decisions, which often vary based on factors like mineral prices, operational costs, and company risk tolerance. Accuracy lacks granularity, treating all incorrect predictions equally regardless of how close they are to the true value. In mineral exploration or grade estimation, predictions that are close to the true value can still be valuable, even if not exactly correct. Accuracy doesn't distinguish between different types of errors. False positives (predicting a viable deposit where there isn't one) and false negatives (missing an existing deposit) have different economic and operational consequences in mining, but accuracy treats them identically. For continuous variables common in mining (e.g., ore grade, geotechnical properties, processing recovery rates), binning into categories for accuracy calculation results in loss of valuable information. These limitations highlight why accuracy alone is often insufficient for evaluating predictive models in mining applications. More nuanced metrics and evaluation approaches are typically needed to capture the full value and implications of model predictions in this complex and high-stakes industry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUC - ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an XGBoost classifier for RFE\n",
    "base_model = XGBClassifier(max_depth=3, n_estimators=50, learning_rate=0.01)\n",
    "\n",
    "# Perform RFE once to rank all features\n",
    "rfe = RFE(estimator=base_model, n_features_to_select=1)\n",
    "rfe.fit(X_train, y_train)\n",
    "\n",
    "# Get the ranking of features\n",
    "feature_ranking = rfe.ranking_\n",
    "\n",
    "# Sort features by their RFE ranking\n",
    "rfe_sorted_features = [x for _, x in sorted(zip(feature_ranking, X_train.columns))]\n",
    "\n",
    "# Calculate the AUC-ROC for different numbers of features using cross-validation\n",
    "n_features = range(1, X_train.shape[1] + 1)\n",
    "auc_scores = []\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "print(\"Number of features | AUC-ROC | Features (RFE order)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for n in n_features:\n",
    "    # Select top n features based on RFE ranking\n",
    "    selected_features = rfe_sorted_features[:n]\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    cv_scores = []\n",
    "    for train_idx, val_idx in cv.split(X_train, y_train):\n",
    "        X_cv_train, X_cv_val = X_train.iloc[train_idx][selected_features], X_train.iloc[val_idx][selected_features]\n",
    "        y_cv_train, y_cv_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "        \n",
    "        model = XGBClassifier()\n",
    "        model.fit(X_cv_train, y_cv_train)\n",
    "        y_pred_proba = model.predict_proba(X_cv_val)[:, 1]\n",
    "        cv_scores.append(roc_auc_score(y_cv_val, y_pred_proba))\n",
    "    \n",
    "    mean_auc_score = np.mean(cv_scores)\n",
    "    auc_scores.append(mean_auc_score)\n",
    "    \n",
    "    print(f\"{n:^17} | {mean_auc_score:.4f} | {', '.join(selected_features)}\")\n",
    "\n",
    "# Create subplots in a 2x2 grid\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(20, 20))\n",
    "\n",
    "# Plot RFE feature ranking\n",
    "ax1.barh(range(len(rfe_sorted_features)), range(len(rfe_sorted_features), 0, -1))\n",
    "ax1.set_yticks(range(len(rfe_sorted_features)))\n",
    "ax1.set_yticklabels(rfe_sorted_features)\n",
    "ax1.set_xlabel('RFE Ranking (lower is better)')\n",
    "ax1.set_ylabel('Feature')\n",
    "ax1.set_title('RFE Feature Ranking')\n",
    "ax1.invert_yaxis()  # Invert y-axis to display best ranked at the top\n",
    "\n",
    "# Plot XGBoost feature importance\n",
    "final_model = XGBClassifier()\n",
    "final_model.fit(X_train, y_train)\n",
    "xgb_feature_importances = final_model.feature_importances_\n",
    "xgb_sorted_idx = np.argsort(xgb_feature_importances)[::-1]\n",
    "xgb_sorted_features = [X_train.columns[i] for i in xgb_sorted_idx]\n",
    "xgb_sorted_importances = xgb_feature_importances[xgb_sorted_idx]\n",
    "\n",
    "ax2.barh(range(len(xgb_sorted_importances)), xgb_sorted_importances)\n",
    "ax2.set_yticks(range(len(xgb_sorted_importances)))\n",
    "ax2.set_yticklabels(xgb_sorted_features)\n",
    "ax2.set_xlabel('Importance')\n",
    "ax2.set_ylabel('Feature')\n",
    "ax2.set_title('XGBoost Feature Importance (from final model)')\n",
    "ax2.invert_yaxis()  # Invert y-axis to display largest importance at the top\n",
    "\n",
    "# Plot AUC-ROC vs number of features with more x-axis ticks\n",
    "ax3.plot(n_features, auc_scores)\n",
    "ax3.scatter(n_features, auc_scores, alpha=0.4)\n",
    "ax3.set_xlabel('Number of features')\n",
    "ax3.set_ylabel('AUC-ROC')\n",
    "ax3.set_title('AUC-ROC vs. Number of features (RFE order)')\n",
    "\n",
    "# Set x-axis ticks to show every feature\n",
    "ax3.set_xticks(n_features)\n",
    "ax3.set_xticklabels(n_features)\n",
    "\n",
    "# Add minor ticks for better readability\n",
    "ax3.set_xticks(n_features, minor=True)\n",
    "ax3.grid(True, linestyle='--', alpha=0.7, which='both')\n",
    "\n",
    "# Plot ROC curves for specific numbers of features\n",
    "ax4.plot([0, 1], [0, 1], linestyle='--', label='Random Classifier')\n",
    "colors = ['red', 'green', 'blue', 'purple']\n",
    "feature_subsets = [1, 3, 7, len(rfe_sorted_features)]\n",
    "\n",
    "for n, color in zip(feature_subsets, colors):\n",
    "    selected_features = rfe_sorted_features[:n]\n",
    "    model = XGBClassifier()\n",
    "    model.fit(X_train[selected_features], y_train)\n",
    "    y_pred_proba = model.predict_proba(X_test[selected_features])[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    ax4.plot(fpr, tpr, color=color, label=f'{n} features (AUC = {auc:.3f})')\n",
    "\n",
    "ax4.set_xlabel('False Positive Rate')\n",
    "ax4.set_ylabel('True Positive Rate')\n",
    "ax4.set_title('ROC Curves for Different Numbers of Features')\n",
    "ax4.legend(loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print AUC-ROC scores for specific feature subsets\n",
    "print(\"\\nAUC-ROC scores for specific feature subsets:\")\n",
    "for n in feature_subsets:\n",
    "    selected_features = rfe_sorted_features[:n]\n",
    "    model = XGBClassifier()\n",
    "    model.fit(X_train[selected_features], y_train)\n",
    "    y_pred_proba = model.predict_proba(X_test[selected_features])[:, 1]\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    print(f\"{n} features: {auc:.4f}\")\n",
    "\n",
    "# Print the order of feature selection by RFE\n",
    "print(\"\\nOrder of feature selection by RFE:\")\n",
    "for i, feature in enumerate(rfe_sorted_features, 1):\n",
    "    print(f\"{i}. {feature}\")\n",
    "\n",
    "# Print the order of feature importance by XGBoost\n",
    "print(\"\\nOrder of feature importance by XGBoost:\")\n",
    "for i, feature in enumerate(xgb_sorted_features, 1):\n",
    "    print(f\"{i}. {feature}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an XGBoost classifier for RFE\n",
    "base_model = XGBClassifier(max_depth=3, n_estimators=50, learning_rate=0.01)\n",
    "\n",
    "# Perform RFE once to rank all features\n",
    "rfe = RFE(estimator=base_model, n_features_to_select=1)\n",
    "rfe.fit(X_train, y_train)\n",
    "\n",
    "# Get the ranking of features\n",
    "feature_ranking = rfe.ranking_\n",
    "\n",
    "# Sort features by their RFE ranking\n",
    "rfe_sorted_features = [x for _, x in sorted(zip(feature_ranking, X_train.columns))]\n",
    "\n",
    "# Calculate the F1 score for different numbers of features using cross-validation\n",
    "n_features = range(1, X_train.shape[1] + 1)\n",
    "f1_scores = []\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "print(\"Number of features | F1 Score | Features (RFE order)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for n in n_features:\n",
    "    # Select top n features based on RFE ranking\n",
    "    selected_features = rfe_sorted_features[:n]\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    cv_scores = []\n",
    "    for train_idx, val_idx in cv.split(X_train, y_train):\n",
    "        X_cv_train, X_cv_val = X_train.iloc[train_idx][selected_features], X_train.iloc[val_idx][selected_features]\n",
    "        y_cv_train, y_cv_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "        \n",
    "        model = XGBClassifier()\n",
    "        model.fit(X_cv_train, y_cv_train)\n",
    "        y_pred = model.predict(X_cv_val)\n",
    "        cv_scores.append(f1_score(y_cv_val, y_pred))\n",
    "    \n",
    "    mean_f1_score = np.mean(cv_scores)\n",
    "    f1_scores.append(mean_f1_score)\n",
    "    \n",
    "    print(f\"{n:^17} | {mean_f1_score:.4f} | {', '.join(selected_features)}\")\n",
    "\n",
    "# Create subplots in a 2x2 grid\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(20, 20))\n",
    "\n",
    "# Plot RFE feature ranking\n",
    "ax1.barh(range(len(rfe_sorted_features)), range(len(rfe_sorted_features), 0, -1))\n",
    "ax1.set_yticks(range(len(rfe_sorted_features)))\n",
    "ax1.set_yticklabels(rfe_sorted_features)\n",
    "ax1.set_xlabel('RFE Ranking (lower is better)')\n",
    "ax1.set_ylabel('Feature')\n",
    "ax1.set_title('RFE Feature Ranking')\n",
    "ax1.invert_yaxis()  # Invert y-axis to display best ranked at the top\n",
    "\n",
    "# Plot XGBoost feature importance\n",
    "final_model = XGBClassifier()\n",
    "final_model.fit(X_train, y_train)\n",
    "xgb_feature_importances = final_model.feature_importances_\n",
    "xgb_sorted_idx = np.argsort(xgb_feature_importances)[::-1]\n",
    "xgb_sorted_features = [X_train.columns[i] for i in xgb_sorted_idx]\n",
    "xgb_sorted_importances = xgb_feature_importances[xgb_sorted_idx]\n",
    "\n",
    "ax2.barh(range(len(xgb_sorted_importances)), xgb_sorted_importances)\n",
    "ax2.set_yticks(range(len(xgb_sorted_importances)))\n",
    "ax2.set_yticklabels(xgb_sorted_features)\n",
    "ax2.set_xlabel('Importance')\n",
    "ax2.set_ylabel('Feature')\n",
    "ax2.set_title('XGBoost Feature Importance (from final model)')\n",
    "ax2.invert_yaxis()  # Invert y-axis to display largest importance at the top\n",
    "\n",
    "# Plot F1 score vs number of features with more x-axis ticks\n",
    "ax3.plot(n_features, f1_scores)\n",
    "ax3.scatter(n_features, f1_scores, alpha=0.4)\n",
    "ax3.set_xlabel('Number of features')\n",
    "ax3.set_ylabel('F1 Score')\n",
    "ax3.set_title('F1 Score vs. Number of features (RFE order)')\n",
    "\n",
    "# Set x-axis ticks to show every feature\n",
    "ax3.set_xticks(n_features)\n",
    "ax3.set_xticklabels(n_features)\n",
    "\n",
    "# Add minor ticks for better readability\n",
    "ax3.set_xticks(n_features, minor=True)\n",
    "ax3.grid(True, linestyle='--', alpha=0.7, which='both')\n",
    "\n",
    "# Plot Precision-Recall curves for specific numbers of features\n",
    "colors = ['red', 'green', 'blue', 'purple']\n",
    "total_features = len(rfe_sorted_features)\n",
    "feature_subsets = [1, total_features // 2, total_features - 1, total_features]\n",
    "\n",
    "for n, color in zip(feature_subsets, colors):\n",
    "    selected_features = rfe_sorted_features[:n]\n",
    "    model = XGBClassifier()\n",
    "    model.fit(X_train[selected_features], y_train)\n",
    "    y_pred_proba = model.predict_proba(X_test[selected_features])[:, 1]\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "    f1 = f1_score(y_test, model.predict(X_test[selected_features]))\n",
    "    ax4.plot(recall, precision, color=color, label=f'{n} features (F1 = {f1:.3f})')\n",
    "\n",
    "ax4.set_xlabel('Recall')\n",
    "ax4.set_ylabel('Precision')\n",
    "ax4.set_title('Precision-Recall Curves for Different Numbers of Features')\n",
    "ax4.legend(loc='lower left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print F1 scores for specific feature subsets\n",
    "print(\"\\nF1 scores for specific feature subsets:\")\n",
    "for n in feature_subsets:\n",
    "    selected_features = rfe_sorted_features[:n]\n",
    "    model = XGBClassifier()\n",
    "    model.fit(X_train[selected_features], y_train)\n",
    "    y_pred = model.predict(X_test[selected_features])\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    print(f\"{n} features: {f1:.4f}\")\n",
    "\n",
    "# Print the order of feature selection by RFE\n",
    "print(\"\\nOrder of feature selection by RFE:\")\n",
    "for i, feature in enumerate(rfe_sorted_features, 1):\n",
    "    print(f\"{i}. {feature}\")\n",
    "\n",
    "# Print the order of feature importance by XGBoost\n",
    "print(\"\\nOrder of feature importance by XGBoost:\")\n",
    "for i, feature in enumerate(xgb_sorted_features, 1):\n",
    "    print(f\"{i}. {feature}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an XGBoost classifier for RFE\n",
    "base_model = XGBClassifier(max_depth=3, n_estimators=50, learning_rate=0.01)\n",
    "\n",
    "# Perform RFE once to rank all features\n",
    "rfe = RFE(estimator=base_model, n_features_to_select=1)\n",
    "rfe.fit(X_train, y_train)\n",
    "\n",
    "# Get the ranking of features\n",
    "feature_ranking = rfe.ranking_\n",
    "\n",
    "# Sort features by their RFE ranking\n",
    "rfe_sorted_features = [x for _, x in sorted(zip(feature_ranking, X_train.columns))]\n",
    "\n",
    "# Calculate the F1 score for different numbers of features using cross-validation\n",
    "n_features = range(1, X_train.shape[1] + 1)\n",
    "f1_scores = []\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "print(\"Number of features | F1 Score (CV) | Features (RFE order)\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for n in n_features:\n",
    "    # Select top n features based on RFE ranking\n",
    "    selected_features = rfe_sorted_features[:n]\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    cv_scores = cross_val_score(XGBClassifier(), X_train[selected_features], y_train, \n",
    "                                cv=cv, scoring='f1')\n",
    "    mean_f1_score = np.mean(cv_scores)\n",
    "    f1_scores.append(mean_f1_score)\n",
    "    \n",
    "    print(f\"{n:^17} | {mean_f1_score:.4f} | {', '.join(selected_features)}\")\n",
    "\n",
    "# Create subplots in a 2x2 grid\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(20, 20))\n",
    "\n",
    "# Plot RFE feature ranking\n",
    "ax1.barh(range(len(rfe_sorted_features)), range(len(rfe_sorted_features), 0, -1))\n",
    "ax1.set_yticks(range(len(rfe_sorted_features)))\n",
    "ax1.set_yticklabels(rfe_sorted_features)\n",
    "ax1.set_xlabel('RFE Ranking (lower is better)')\n",
    "ax1.set_ylabel('Feature')\n",
    "ax1.set_title('RFE Feature Ranking')\n",
    "ax1.invert_yaxis()  # Invert y-axis to display best ranked at the top\n",
    "\n",
    "# Plot XGBoost feature importance\n",
    "final_model = XGBClassifier()\n",
    "final_model.fit(X_train, y_train)\n",
    "xgb_feature_importances = final_model.feature_importances_\n",
    "xgb_sorted_idx = np.argsort(xgb_feature_importances)[::-1]\n",
    "xgb_sorted_features = [X_train.columns[i] for i in xgb_sorted_idx]\n",
    "xgb_sorted_importances = xgb_feature_importances[xgb_sorted_idx]\n",
    "\n",
    "ax2.barh(range(len(xgb_sorted_importances)), xgb_sorted_importances)\n",
    "ax2.set_yticks(range(len(xgb_sorted_importances)))\n",
    "ax2.set_yticklabels(xgb_sorted_features)\n",
    "ax2.set_xlabel('Importance')\n",
    "ax2.set_ylabel('Feature')\n",
    "ax2.set_title('XGBoost Feature Importance (from final model)')\n",
    "ax2.invert_yaxis()  # Invert y-axis to display largest importance at the top\n",
    "\n",
    "# Plot F1 score vs number of features with more x-axis ticks\n",
    "ax3.plot(n_features, f1_scores)\n",
    "ax3.scatter(n_features, f1_scores, alpha=0.4)\n",
    "ax3.set_xlabel('Number of features')\n",
    "ax3.set_ylabel('F1 Score (Cross-validated)')\n",
    "ax3.set_title('F1 Score vs. Number of features (RFE order)')\n",
    "\n",
    "# Set x-axis ticks to show every feature\n",
    "ax3.set_xticks(n_features)\n",
    "ax3.set_xticklabels(n_features)\n",
    "\n",
    "# Add minor ticks for better readability\n",
    "ax3.set_xticks(n_features, minor=True)\n",
    "ax3.invert_xaxis()\n",
    "ax3.grid(True, linestyle='--', alpha=0.7, which='both')\n",
    "\n",
    "# Plot Precision-Recall curves for specific numbers of features\n",
    "colors = ['red', 'green', 'blue', 'purple', 'orange']\n",
    "total_features = len(rfe_sorted_features)\n",
    "feature_subsets = [1, 3, total_features // 2, total_features - 1, total_features]\n",
    "\n",
    "print(\"\\nDetailed F1 scores for specific feature subsets:\")\n",
    "print(\"Subset | Train F1 (CV) | Test F1 | Features\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for n, color in zip(feature_subsets, colors):\n",
    "    selected_features = rfe_sorted_features[:n]\n",
    "    \n",
    "    # Cross-validation on training set\n",
    "    cv_scores = cross_val_score(XGBClassifier(), X_train[selected_features], y_train, \n",
    "                                cv=5, scoring='f1')\n",
    "    mean_cv_f1 = np.mean(cv_scores)\n",
    "    \n",
    "    # Fit on entire training set and evaluate on test set\n",
    "    model = XGBClassifier()\n",
    "    model.fit(X_train[selected_features], y_train)\n",
    "    y_pred = model.predict(X_test[selected_features])\n",
    "    test_f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    # Plot Precision-Recall curve\n",
    "    y_pred_proba = model.predict_proba(X_test[selected_features])[:, 1]\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "    ax4.plot(recall, precision, color=color, \n",
    "             label=f'{n} features (Test F1 = {test_f1:.3f})')\n",
    "    \n",
    "    print(f\"{n:^6} | {mean_cv_f1:.4f} | {test_f1:.4f} | {', '.join(selected_features)}\")\n",
    "\n",
    "ax4.set_xlabel('Recall')\n",
    "ax4.set_ylabel('Precision')\n",
    "ax4.set_title('Precision-Recall Curves for Different Numbers of Features')\n",
    "ax4.legend(loc='lower left')\n",
    "\n",
    "plt.tight_layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
